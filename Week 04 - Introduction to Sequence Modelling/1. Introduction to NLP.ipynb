{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction to NLP\n",
        "\n",
        "<!--<badge>--><a href=\"https://colab.research.google.com/github/TheAIDojo/AI_4_Climate_Bootcamp/blob/main/Week 04 - Introduction to Sequence Modelling/1. Introduction to NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a><!--</badge>-->\n",
        "\n",
        "Natural Language Processing (NLP) is a field that focuses on the interaction between computers and human language. It deals with the analysis, understanding and generation of human language in a meaningful and useful manner.\n",
        "\n",
        "One of the applications of NLP is sentiment analysis, which is the task of determining whether a given piece of text has a positive, negative or neutral sentiment. Another application is machine translation, which is the task of translating one language into another.\n",
        "\n",
        "TensorFlow and Keras are popular open-source tools used in NLP, which are both highly customizable and flexible. These tools allow you to build and train deep learning models for NLP tasks such as sentiment analysis, text classification and text generation.\n",
        "\n",
        "Pandas, Matplotlib and Numpy are data analysis libraries that are also commonly used in NLP. They allow you to manipulate and visualize data in a meaningful and useful manner.\n",
        "\n",
        "For further reading on NLP and its applications, check out the following resources:\n",
        "- [TensorFlow NLP Tutorials](https://www.tensorflow.org/tutorials/text)\n",
        "- [Keras NLP Guide](https://keras.io/guides/keras_nlp/getting_started/)\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Table of Contents <a name=\"toc\"></a>\n",
        "- [Text Preprocessing](#text-preprocessing)\n",
        "  - [Text Cleaning](#text-cleaning)\n",
        "  - [Text Tokenization](#text-tokenization)\n",
        "  - [Text Padding](#text-padding)\n",
        "- [Dense Model Training](#dense-model)\n",
        "- [Word Embeddings](#word-embeddings)\n",
        "  - [Embedding Model](#embedding-model)\n",
        "- [Recurrent Neural Networks (RNNs)](#rnn)\n",
        "  - [Simple RNN Model](#simple-rnn-model)\n",
        "  - [LSTM Model](#lstm-model)\n",
        "  - [GRU Model](#gru-model)\n",
        "  - [Bidirectional RNN Model](#bidirectional-rnn-model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import libaries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn import model_selection\n",
        "from nltk.corpus import (\n",
        "    stopwords,\n",
        ")  # stopwords module from NLTK (Natural Language Toolkit)\n",
        "import re  # built-in regular expression module\n",
        "import string  # built-in string module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# download stopwords\n",
        "nltk.download(\"stopwords\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Download Dataset from Kaggle\n",
        "\n",
        "Before running the celll below, make sure you upload your Kaggle API token to the notebook. You can do this by clicking on the \"Files\" tab on the left, then clicking on the \"Upload\" button. You can find your Kaggle API token by going to your Kaggle account, clicking on \"My Account\", then clicking on \"Create New API Token\". You can then upload the \"kaggle.json\" file to the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# upload kaggle.json into this folder before running this command\n",
        "!mkdir /root/.kaggle\n",
        "!cp kaggle.json /root/.kaggle/kaggle.json\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
        "!unzip imdb-dataset-of-50k-movie-reviews.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# read dataset from csv file\n",
        "df = pd.read_csv(\"IMDB Dataset.csv\")\n",
        "df.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text Preprocessing <a name=\"text-preprocessing\"></a>\n",
        "[Back to Top](#toc)\n",
        "\n",
        "Before training our NLP models, it is necessary to preprocess the text data so that it is in a format that can be easily understood by the models. There are several steps involved in text preprocessing, including text cleanup, tokenization, and padding.\n",
        "\n",
        "- Text cleanup is the process of removing any unwanted elements from the text, such as punctuation, stop words, numbers, and special characters. This step is important because these elements can often introduce noise into the data and negatively impact the performance of the model. Some common text cleanup tasks include:\n",
        "  - Removing punctuation\n",
        "  - Removing stop words\n",
        "  - Removing numbers\n",
        "  - Removing special characters\n",
        "  - Lowercasing the text\n",
        "\n",
        "- Tokenization is the process of converting a piece of text into individual tokens, which are typically words or phrases. This step is important because it allows the model to work with individual words or phrases rather than with the entire text.\n",
        "\n",
        "- Padding is the process of adding padding tokens to the text so that all the text sequences have the same length. This step is important because many NLP models require fixed input lengths.\n",
        "\n",
        "Let's go through them step by step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "# first, we will set some parameters\n",
        "vocab_size = 8000  # number of words in the vocabulary, we will use the top 8000 most common words\n",
        "max_length = 120  # maximum length of a review, we will truncate reviews longer than 120 words and pad reviews shorter than 120 words\n",
        "embedding_dim = 50  # dimension of the embedding vector, we will use 50-dimensional embedding vectors\n",
        "batch_size = 32  # number of reviews in each batch\n",
        "seed = 42  # random seed"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Text Cleanup <a name=\"text-cleanup\"></a>\n",
        "[Back to Top](#toc)\n",
        "\n",
        "Text cleanup is an important step in text preprocessing that removes any unwanted elements from the text, such as punctuation, stop words, numbers, and special characters. The motivation behind text cleanup is to reduce noise in the data and improve the performance of the model.\n",
        "\n",
        "There are several tasks involved in text cleanup, including:\n",
        "- Removing punctuation\n",
        "- Removing stop words\n",
        "- Removing numbers\n",
        "- Removing special characters\n",
        "- Lowercasing the text\n",
        "  \n",
        "\n",
        "It is important to consider which text cleanup tasks are necessary for your specific use case. For example, if you are performing sentiment analysis, it may not be necessary to remove numbers from the text. On the other hand, if you are performing machine translation, it may be necessary to remove special characters from the text.\n",
        "\n",
        "In the next cells, we will show you how to perform text cleanup in Python using TensorFlow and Keras.\n",
        "\n",
        "For comprehensive text cleanup functions, check out the following code:\n",
        "- [English Text Cleanup](https://github.com/jfilter/clean-text/blob/main/cleantext/clean.py)\n",
        "- [Arabic Text Cleanup](https://github.com/ARBML/tnkeeh/blob/master/tnkeeh/tnkeeh.py)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>one of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>a wonderful little production the filming tech...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>i thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>basically there s a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>petter mattei s love in the time of money is a...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  one of the other reviewers has mentioned that ...  positive\n",
              "1  a wonderful little production the filming tech...  positive\n",
              "2  i thought this was a wonderful way to spend ti...  positive\n",
              "3  basically there s a family where a little boy ...  negative\n",
              "4  petter mattei s love in the time of money is a...  positive"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def text_cleanup(text, remove_stopwords=False):\n",
        "    # change all text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # remove HTML tags\n",
        "    text = re.sub(r\"<.*?>\", \"\", text)\n",
        "\n",
        "    # remove numbers\n",
        "    text = re.sub(r\"\\d+\", \"\", text)\n",
        "\n",
        "    # remove words with numbers\n",
        "    text = re.sub(r\"\\w*\\d\\w*\", \"\", text)\n",
        "\n",
        "    # remove URLs\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)\n",
        "\n",
        "    # remove emails\n",
        "    text = re.sub(r\"\\S*@\\S*\\s?\", \"\", text)\n",
        "\n",
        "    # remove mentions (@username)\n",
        "    text = re.sub(r\"@\\S+\", \"\", text)\n",
        "\n",
        "    # remove hashtags (#)\n",
        "    text = re.sub(r\"#\\S+\", \"\", text)\n",
        "\n",
        "    # remove Punctuation\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "\n",
        "    # remove extra spaces\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "\n",
        "    # note that removing stopwords is optional and depends on the task, in some tasks removing stopwords can be beneficial, in other tasks removing stopwords can be harmful.\n",
        "    # as a rule of thumb, you should try both options and see which one works better for your task.\n",
        "\n",
        "    if remove_stopwords:\n",
        "        # remove stopwords (the, a, an, etc.)\n",
        "        stop_words = set(stopwords.words(\"english\"))\n",
        "        words = word_tokenize(text)\n",
        "        words = [word for word in words if not word in stop_words]\n",
        "        text = \" \".join(words)\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "# apply text_cleanup function to the reviews\n",
        "df[\"review\"] = df[\"review\"].map(text_cleanup)\n",
        "df.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Text Tokenization <a name=\"tokenization\"></a>\n",
        "[Back to Top](#toc)\n",
        "\n",
        "Text tokenization is the process of converting a sentence or document into tokens, or meaningful units of words. This is an important step in preparing text data for training machine learning models.\n",
        "\n",
        "In Keras and TensorFlow, text tokenization can be performed using the Tokenizer class. This class has several important parameters:\n",
        "\n",
        "- `num_words`: The maximum number of words to keep, based on word frequency. Only the most common `num_words` will be kept, and all other words will be set to an Out-of-Vocabulary (OOV) token.\n",
        "- `oov_token`: The string that will be used to represent OOV words.\n",
        "- `filters`: A string of characters to filter out, for example punctuation.\n",
        "- `lower`: A flag to convert all text to lowercase before tokenization.\n",
        "\n",
        "Let's see how to use the Tokenizer class to tokenize text in Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'<OOV>': 1, 'the': 2, 'and': 3, 'a': 4, 'of': 5, 'to': 6, 'is': 7, 'it': 8, 'in': 9, 'i': 10}\n"
          ]
        }
      ],
      "source": [
        "# create tokenizer object\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "    num_words=vocab_size, oov_token=\"<OOV>\"\n",
        ")\n",
        "\n",
        "# fit tokenizer on the reviews\n",
        "tokenizer.fit_on_texts(df[\"review\"])\n",
        "\n",
        "# preview the word index, notice that the word index is sorted by frequency\n",
        "word_index = tokenizer.word_index\n",
        "print({k: word_index[k] for k in list(word_index)[:10]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[11, 7, 4, 1, 3024, 8, 1372, 49, 663, 12, 26, 2596, 3, 49, 663, 12, 26, 24, 2596]]\n"
          ]
        }
      ],
      "source": [
        "# let's see how the tokenizer works\n",
        "text = \"This is a sample text, it contains some words that are repeated, and some words that are not repeated.\"\n",
        "sequence = tokenizer.texts_to_sequences([text])\n",
        "print(sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[29, 5, 2, 78, 2040, 47, 1051, 12, 101, 150, 42, 3068, 395, 21, 231, 30, 3173, 33, 26, 204, 15, 11, 7, 615, 48, 591, 18, 69, 2, 88, 149, 12, 3217, 69, 45, 3068, 14, 92, 5323, 3, 1, 136, 5, 561, 62, 266, 9, 204, 38, 2, 649, 142, 1720, 69, 11, 7, 24, 4, 117, 17, 2, 7805, 2309, 41, 1, 11, 117, 2572, 57, 5843, 18, 5465, 6, 1454, 372, 41, 561, 92, 7, 3784, 9, 2, 356, 357, 5, 2, 649, 8, 7, 433, 3068, 15, 12, 7, 2, 1, 358, 6, 2, 1, 6785, 2543, 1032, 1, 8, 2683, 1398, 23, 1, 520, 35, 4638, 2437, 5, 2, 1182, 116, 31, 2, 6924, 28, 2884, 1, 3, 386, 1, 37, 1, 7, 24, 298, 23, 2, 4848, 2914, 520, 7, 342, 6, 108, 1, 1, 1, 1, 4989, 7684, 2425, 3, 53, 37, 1, 325, 1, 7229, 1, 3, 1, 1, 26, 112, 224, 241, 10, 61, 133, 2, 281, 1312, 5, 2, 117, 7, 682, 6, 2, 193, 12, 8, 267, 116, 78, 274, 574, 22, 2993, 819, 183, 1289, 4125, 17, 2473, 1211, 819, 1419, 819, 865, 3068, 153, 22, 940, 185, 2, 88, 395, 10, 124, 210, 3217, 69, 15, 37, 1604, 8, 14, 2220, 10, 412, 22, 133, 10, 14, 1567, 17, 8, 19, 15, 10, 291, 53, 10, 1402, 4, 1254, 17, 3068, 3, 191, 1, 6, 2, 298, 2019, 5, 2115, 561, 24, 42, 561, 19, 7568, 7070, 4957, 36, 231, 30, 2956, 44, 17, 4, 1, 6786, 36, 231, 496, 23, 630, 3, 76, 241, 18, 8, 70, 7510, 640, 695, 6786, 110, 650, 84, 1182, 1, 682, 6, 67, 566, 5, 891, 2001, 41, 1182, 551, 150, 3068, 21, 198, 426, 3802, 18, 48, 7, 3287, 795, 1581, 46, 21, 51, 76, 9, 1200, 18, 127, 4059, 480]\n",
            "[4, 391, 121, 351, 2, 1362, 2950, 7, 54, 1, 54, 159, 58, 2145, 1568, 3, 410, 4, 1, 3, 527, 1, 282, 5, 1817, 6, 2, 440, 411, 2, 151, 26, 559, 70, 2251, 482, 4126, 24, 63, 47, 191, 31, 2, 1, 19, 25, 47, 31, 2, 2263, 177, 3249, 98, 21, 51, 368, 66, 2, 1, 779, 1, 34, 2, 1820, 6, 1713, 7276, 6508, 24, 63, 7, 8, 70, 277, 2, 150, 19, 8, 7, 4, 1, 405, 3, 2372, 411, 4, 4292, 351, 45, 29, 5, 2, 81, 1130, 13, 5, 201, 3, 27, 114, 2, 1817, 65, 269, 342, 18, 2, 121, 180, 2, 1007, 5, 2, 2874, 62, 249, 73, 357, 2, 2179, 953, 3102, 1268, 1174, 94, 4874, 8, 299, 23, 257, 1815, 3, 257, 4511, 575, 18, 2, 136, 3665, 1, 3, 1, 3, 2, 719, 575, 5, 67, 1036, 18, 1, 13, 1, 1, 172, 2270, 26, 1952, 70, 220]\n",
            "[10, 192, 11, 14, 4, 391, 96, 6, 1138, 58, 23, 4, 98, 857, 1452, 2573, 1223, 9, 2, 885, 1, 758, 3, 150, 4, 635, 2309, 201, 2, 113, 7, 4060, 19, 2, 409, 7, 1890, 3, 2, 102, 26, 1468, 60, 2, 70, 6004, 6596, 1538, 471, 137, 49, 198, 30, 669, 52, 33, 946, 11, 7, 24, 1013, 219, 2893, 5100, 10, 192, 8, 14, 3017, 12, 2811, 1768, 7, 132, 1385, 9, 1117, 5, 2, 394, 108, 5, 179, 28, 2041, 6, 111, 11, 14, 2, 90, 10, 223, 1426, 32, 29, 5, 2811, 13, 1292, 9, 155, 2993, 10, 133, 4, 2042, 137, 10, 138, 112, 77, 1489, 18, 1, 1, 9, 11, 55, 1310, 6, 1238, 177, 43, 1231, 1405, 3, 5030, 204, 84, 4, 831, 19, 3516, 187, 246, 11, 198, 24, 30, 2, 7119, 5051, 5, 27, 616, 19, 8, 14, 1, 73, 1798, 2908, 1, 3, 53, 218, 73, 2486, 4, 81, 201, 6, 142, 66, 18, 349]\n",
            "[678, 40, 13, 4, 227, 116, 4, 121, 398, 3319, 1234, 40, 13, 4, 984, 9, 27, 4178, 27, 759, 26, 982, 31, 2, 58, 11, 16, 7, 7743, 73, 4, 1812, 1279, 3, 1095, 3319, 1072, 6, 426, 5409, 3, 496, 2, 984, 568, 88, 5, 31, 52, 21, 152, 168, 6, 95, 4, 20, 21, 203, 1148, 46, 92, 4, 689, 41, 4, 444, 15, 4, 444, 2, 16, 7, 1688, 759, 26, 1, 6819, 39, 9, 146, 114, 3, 94, 71, 28, 3319, 18, 27, 4178, 62, 453, 4073, 31, 2, 20, 10, 846, 6, 66, 4, 1, 731, 16, 3, 301, 10, 291, 4, 444, 18, 49, 3834, 689, 3250, 44, 5, 42, 17, 2, 70, 393, 759, 4595, 3279, 15, 17, 2, 633, 18, 3319, 42, 2748, 93]\n",
            "[1, 1, 13, 111, 9, 2, 58, 5, 290, 7, 4, 2116, 1407, 20, 6, 104, 436, 1, 1549, 179, 4, 4249, 3280, 45, 390, 4432, 11, 7, 4, 16, 12, 186, 6, 30, 979, 179, 48, 290, 639, 3, 988, 79, 6, 80, 9, 2, 278, 1123, 71, 2462, 11, 110, 4, 7016, 23, 2, 1754, 1, 13, 286, 45, 2, 169, 757, 2, 154, 1, 2, 205, 6, 2, 978, 58, 170, 833, 116, 31, 134, 278, 102, 887, 3, 3742, 252, 29, 7, 3230, 9, 29, 96, 41, 160, 6, 2, 364, 388, 19, 57, 29, 186, 6, 120, 2, 916, 219, 5, 2875, 1, 2, 20, 47, 4, 3357, 1, 166, 71, 26, 599, 6, 66, 86, 134, 80, 418, 3, 2, 176, 33, 418, 9, 67, 199, 1, 2, 63, 149, 29, 217, 44, 5, 31, 134, 3363, 9, 2, 432, 7, 2, 278, 5725, 5, 4746, 252, 29, 1, 4, 190, 520, 7, 24, 615, 2, 119, 273, 9, 62, 390, 4432, 167, 4463, 1, 15, 29, 1, 7, 2, 414, 18, 90, 5, 2, 80, 71, 2462, 2, 115, 7, 50, 457, 436, 1, 13, 458, 1284, 1, 1, 5101, 3571, 2666, 482, 1, 6977, 1, 3, 2, 367, 5, 2, 1009, 178, 95, 134, 102, 214, 1099, 71, 629, 436, 1, 50, 2015, 3, 1, 1, 17, 27, 364, 161]\n"
          ]
        }
      ],
      "source": [
        "# now let's convert the reviews to sequences\n",
        "sequences = tokenizer.texts_to_sequences(df[\"review\"])\n",
        "\n",
        "# preview some sequences, notice that the sequences have different lengths\n",
        "for i in range(5):\n",
        "    print(sequences[i])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Padding in NLP\n",
        "\n",
        "In NLP, it is common to have sequences of different lengths, for example, in a sentiment analysis task, you may have reviews of different lengths. However, deep learning models such as Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) require input data to have the same length. Therefore, padding is used to make the sequences have the same length.\n",
        "\n",
        "In TensorFlow and Keras, padding can be done using the `pad_sequences` function from the `keras.preprocessing.sequence` module. The important parameters for this function are:\n",
        "\n",
        "- `sequences`: A list of sequences, where each sequence is a list of integers.\n",
        "- `maxlen`: The maximum length of the sequences. If the length of a sequence is greater than `maxlen`, it will be truncated, and if it is smaller, it will be padded with zeros.\n",
        "- `padding`: The type of padding to use, either `'pre'` or `'post'`. Default is `'pre'`.\n",
        "- `truncating`: The type of truncating to use, either `'pre'` or `'post'`. Default is `'pre'`.\n",
        "\n",
        "Here's an example of how to use the `pad_sequences` function:\n",
        "\n",
        "```python\n",
        "sequences = [[1, 2, 3, 4, 5, 6], [4, 5], [6]]\n",
        "padded_sequences = keras.preprocessing.sequence.pad_sequences(sequences, maxlen=5, padding='post')\n",
        "print(padded_sequences)\n",
        "```\n",
        "\n",
        "The output will be\n",
        "\n",
        "```\n",
        "[[1 2 3 4 5]\n",
        " [4 5 0 0 0]\n",
        " [6 0 0 0 0]]\n",
        "```\n",
        "\n",
        "In this example, all sequences have been padded to have a length of 5, and the padding was done on the right side (post).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Padded Sequences Shape:  (50000, 120)\n"
          ]
        }
      ],
      "source": [
        "# pad the sequences\n",
        "padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    sequences, maxlen=max_length, padding=\"post\"\n",
        ")\n",
        "\n",
        "# print padded_sequences shape, notice that the shape is uniform (batch_size, max_length)\n",
        "print(\"Padded Sequences Shape: \", padded_sequences.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Standard Preprocessing Steps\n",
        "\n",
        "The following preprocessing steps are common to other classification tasks, we'll encoder the labels and split the data into training and testing sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_train shape:  (45000, 120)\n",
            "x_test shape:  (5000, 120)\n",
            "y_train shape:  (45000,)\n",
            "y_test shape:  (5000,)\n"
          ]
        }
      ],
      "source": [
        "# store padded sequences to `x`\n",
        "x = padded_sequences\n",
        "\n",
        "# encode labels to y\n",
        "y = df[\"sentiment\"].map({\"positive\": 1, \"negative\": 0})\n",
        "\n",
        "\n",
        "# split the data into training and testing sets\n",
        "x_train, x_test, y_train, y_test = model_selection.train_test_split(\n",
        "    x, y, test_size=0.1, random_state=seed, stratify=y\n",
        ")\n",
        "\n",
        "print(\"x_train shape: \", x_train.shape)\n",
        "print(\"x_test shape: \", x_test.shape)\n",
        "print(\"y_train shape: \", y_train.shape)\n",
        "print(\"y_test shape: \", y_test.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dense Model Training <a name=\"dense-model\"></a>\n",
        "[Back to Top](#toc)\n",
        "\n",
        "We will train a simple feed-forward (Dense) neural network model to classify the text data. This model will have an input layer, a hidden layer, and an output layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metal device set to: Apple M2 Pro\n",
            "\n",
            "systemMemory: 32.00 GB\n",
            "maxCacheSize: 10.67 GB\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-02-04 19:45:47.191124: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
            "2023-02-04 19:45:47.191301: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 64)                7744      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 9,857\n",
            "Trainable params: 9,857\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "dense_model = tf.keras.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.Dense(64, activation=\"relu\", input_shape=(max_length,)),\n",
        "        tf.keras.layers.Dense(32, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "dense_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-02-04 19:46:12.419406: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
            "2023-02-04 19:46:12.567668: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1407/1407 [==============================] - ETA: 0s - loss: 17.3928 - accuracy: 0.5022"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-02-04 19:46:21.867471: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1407/1407 [==============================] - 10s 6ms/step - loss: 17.3928 - accuracy: 0.5022 - val_loss: 0.8066 - val_accuracy: 0.5046\n",
            "Epoch 2/10\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 0.7126 - accuracy: 0.5026 - val_loss: 0.7659 - val_accuracy: 0.4958\n",
            "Epoch 3/10\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 0.6947 - accuracy: 0.5013 - val_loss: 0.7647 - val_accuracy: 0.5004\n",
            "Epoch 4/10\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 0.6934 - accuracy: 0.5002 - val_loss: 0.7604 - val_accuracy: 0.5022\n",
            "Epoch 5/10\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 0.6936 - accuracy: 0.5014 - val_loss: 0.7472 - val_accuracy: 0.5008\n",
            "Epoch 6/10\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 0.6935 - accuracy: 0.5047 - val_loss: 0.7500 - val_accuracy: 0.4992\n",
            "Epoch 7/10\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 0.6929 - accuracy: 0.4933 - val_loss: 0.7513 - val_accuracy: 0.4996\n",
            "Epoch 8/10\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 0.6932 - accuracy: 0.4976 - val_loss: 0.6966 - val_accuracy: 0.4994\n",
            "Epoch 9/10\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 0.6936 - accuracy: 0.4993 - val_loss: 0.7005 - val_accuracy: 0.5004\n",
            "Epoch 10/10\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 0.6937 - accuracy: 0.4977 - val_loss: 0.7032 - val_accuracy: 0.4994\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x331f435e0>"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# compile the model\n",
        "dense_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# train the model\n",
        "dense_model.fit(\n",
        "    x_train, y_train, epochs=10, batch_size=batch_size, validation_data=(x_test, y_test)\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When using a dense only neural network for sentiment analysis, we are essentially treating the words as numerical values. This means that each word is assigned a unique number, and this number is used to represent the word in the neural network.\n",
        "\n",
        "The problem with this approach is that words are not equally meaningful. For example, the words \"great\" and \"awful\" are highly meaningful in the context of sentiment analysis, while words like \"the\" or \"and\" are not.\n",
        "\n",
        "When using a dense only neural network, all words are treated equally, regardless of their meaning. This means that the network has no way to distinguish between important words and less important words.\n",
        "\n",
        "As a result, a dense only neural network for sentiment analysis will typically perform at random chance (50% accuracy) when evaluated on a large dataset. This is because the network is unable to learn meaningful relationships between words and sentiments.\n",
        "\n",
        "To overcome this limitation, we need to use a more advanced approach such as word embeddings, which will allow the network to learn meaningful relationships between words and sentiments."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Word Embeddings <a name=\"word-embeddings\"></a>\n",
        "[Back to Top](#toc)\n",
        "\n",
        "Word embeddings are a way to represent words in a dense vector representation, where semantically similar words have similar vector representations. The motivation behind using word embeddings is that one-hot encoded representations of words can be very sparse and high-dimensional, making it difficult for a neural network to learn from them.\n",
        "\n",
        "Word embeddings are learned from the text during training and can capture semantic relationships between words. For example, in a word embedding, vectors for words like \"king\", \"queen\", \"prince\", and \"princess\" might be close together, while vectors for words like \"computer\" and \"book\" might be far apart.\n",
        "\n",
        "In Keras, word embeddings can be learned using the Embedding layer. The Embedding layer takes an integer-encoded vocabulary and maps each word to a dense vector representation. The length of the vector representation is a hyperparameter that can be tuned, and the dimensionality of the vectors will be `(vocabulary size, embedding dimension)`.\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Embedding Model <a name=\"embedding-model\"></a>\n",
        "[Back to Top](#toc)\n",
        "\n",
        "We will now train a neural network model that uses word embeddings. This model will have an input layer, an embedding layer, a hidden layer, and an output layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 120, 50)           400000    \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 6000)              0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 64)                384064    \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 786,177\n",
            "Trainable params: 786,177\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "embedding_model = tf.keras.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(64, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(32, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "embedding_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-02-04 19:53:12.399028: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1407/1407 [==============================] - ETA: 0s - loss: 0.3842 - accuracy: 0.8178"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-02-04 19:53:27.742409: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1407/1407 [==============================] - 16s 11ms/step - loss: 0.3842 - accuracy: 0.8178 - val_loss: 0.3140 - val_accuracy: 0.8670\n",
            "Epoch 2/10\n",
            "1407/1407 [==============================] - 12s 9ms/step - loss: 0.1583 - accuracy: 0.9403 - val_loss: 0.4356 - val_accuracy: 0.8486\n",
            "Epoch 3/10\n",
            "1407/1407 [==============================] - 12s 8ms/step - loss: 0.0369 - accuracy: 0.9868 - val_loss: 0.6951 - val_accuracy: 0.8432\n",
            "Epoch 4/10\n",
            "1407/1407 [==============================] - 12s 8ms/step - loss: 0.0201 - accuracy: 0.9931 - val_loss: 0.7439 - val_accuracy: 0.8462\n",
            "Epoch 5/10\n",
            "1407/1407 [==============================] - 12s 8ms/step - loss: 0.0174 - accuracy: 0.9941 - val_loss: 0.8048 - val_accuracy: 0.8428\n",
            "Epoch 6/10\n",
            "1407/1407 [==============================] - 12s 8ms/step - loss: 0.0127 - accuracy: 0.9955 - val_loss: 0.9476 - val_accuracy: 0.8404\n",
            "Epoch 7/10\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.0112 - accuracy: 0.9964 - val_loss: 0.9921 - val_accuracy: 0.8254\n",
            "Epoch 8/10\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.0118 - accuracy: 0.9957 - val_loss: 1.0806 - val_accuracy: 0.8340\n",
            "Epoch 9/10\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.0091 - accuracy: 0.9969 - val_loss: 1.0735 - val_accuracy: 0.8308\n",
            "Epoch 10/10\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.0069 - accuracy: 0.9977 - val_loss: 1.1532 - val_accuracy: 0.8282\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x2ee0891c0>"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# compile the model\n",
        "embedding_model.compile(\n",
        "    optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# train the model\n",
        "embedding_model.fit(\n",
        "    x_train, y_train, epochs=10, batch_size=batch_size, validation_data=(x_test, y_test)\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Despite the improved performance that using word embeddings and dense layers in a neural network model can bring to sentiment analysis tasks, there are still limitations that need to be addressed.\n",
        "\n",
        "A major limitation is that dense layers and embeddings only capture shallow semantic information, meaning that they are not capable of capturing the deeper relationships between words and the context in which they are used. This can be addressed by using other types of layers such as recurrent neural networks (RNNs) or convolutional neural networks (CNNs).\n",
        "\n",
        "Notice this two sentences:\n",
        "```\n",
        "The movie was good, not bad at all.\n",
        "The movie was bad, not good at all.\n",
        "```\n",
        "The first sentence is positive, while the second sentence is negative. However, the words \"good\" and \"bad\" are used in the same way in both sentences, and the network has no way of distinguishing between the two since it is only looking at the words individually without considering the context in which they are used."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Recurrent Neural Networks (RNNs) <a name=\"rnn\"></a>\n",
        "[Back to Top](#toc)\n",
        "\n",
        "Recurrent Neural Networks (RNNs) are a type of neural network designed to handle sequential data. They are commonly used in Natural Language Processing (NLP) tasks such as sentiment analysis, machine translation, and text classification.\n",
        "\n",
        "RNNs are motivated by the fact that traditional neural networks are not well suited for tasks that require processing sequences of data. Unlike feedforward neural networks, RNNs have the ability to retain information from previous time steps and use that information to inform future predictions. This allows them to model the relationships between elements in a sequence.\n",
        "\n",
        "In an RNN, each unit (neuron) receives input from the previous unit and its own input at the current time step. This allows the network to maintain a hidden state that summarizes the information it has processed so far. This hidden state is updated at each time step and is used as input to the next unit in the sequence.\n",
        "\n",
        "To use RNNs in Keras, you can use the `SimpleRNN`, `LSTM`, or `GRU` layers. These layers are similar in that they all process sequences of data, but they differ in their internal mechanisms for retaining information from previous time steps.\n",
        "\n",
        "Note that RNNs expect the input data to be in a specific shape. The input data should be a 3-dimensional array with the shape `(batch size, sequence length, number of features)`. The batch size is the number of samples in the current batch, the sequence length is the number of time steps in each sample (e.g., number of words in a sentences), and the number of features is the number of features in each time step.\n",
        "\n",
        "You can also stack multiple RNN layers on top of each other to create a deep RNN. This can be done by setting `return_sequences=True` when creating the layer. This will cause the layer to return the full sequence of outputs for each sample instead of just the last output. Note that RNNs always expect a sequence as input."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Simple RNN Model <a name=\"simple-rnn-model\"></a>\n",
        "[Back to Top](#toc)\n",
        "\n",
        "We will now train a neural network model that uses a simple RNN layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 120, 50)           400000    \n",
            "                                                                 \n",
            " simple_rnn (SimpleRNN)      (None, 64)                7360      \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 64)                4160      \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 413,633\n",
            "Trainable params: 413,633\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "simple_rnn_model = tf.keras.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "        tf.keras.layers.SimpleRNN(64, activation=\"tanh\"),\n",
        "        tf.keras.layers.Dense(64, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(32, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "simple_rnn_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# compile the model\n",
        "simple_rnn_model.compile(\n",
        "    optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# train the model\n",
        "simple_rnn_model.fit(\n",
        "    x_train, y_train, epochs=10, batch_size=batch_size, validation_data=(x_test, y_test)\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LSTM Model <a name=\"lstm-model\"></a>\n",
        "\n",
        "Recurrent Neural Networks (RNNs) have shown great success in various Natural Language Processing (NLP) tasks such as sentiment analysis, language translation, and text generation. However, traditional RNNs have a limitation in capturing long-term dependencies in sequences, due to the vanishing gradient problem. To overcome this issue, Long Short-Term Memory (LSTM) networks were introduced.\n",
        "\n",
        "LSTMs were designed to tackle the problem of vanishing gradients by introducing memory cells, gates, and a cell state to control the flow of information in the network. This allows LSTMs to capture long-term dependencies, and make predictions based on both recent and historical context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (None, 120, 50)           400000    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 64)                29440     \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 64)                4160      \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 435,713\n",
            "Trainable params: 435,713\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "lstm_model = tf.keras.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "        tf.keras.layers.LSTM(64, activation=\"tanh\"),\n",
        "        tf.keras.layers.Dense(64, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(32, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "lstm_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1407/1407 [==============================] - 44s 31ms/step - loss: 0.0659 - accuracy: 0.9778 - val_loss: 0.6233 - val_accuracy: 0.8484\n",
            "Epoch 10/10\n",
            "1407/1407 [==============================] - 44s 32ms/step - loss: 0.0565 - accuracy: 0.9816 - val_loss: 0.7043 - val_accuracy: 0.8466\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x331aa2910>"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# compile the model\n",
        "lstm_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# train the model\n",
        "lstm_model.fit(\n",
        "    x_train, y_train, epochs=10, batch_size=batch_size, validation_data=(x_test, y_test)\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GRU Model <a name=\"gru-model\"></a>\n",
        "[Back to Top](#toc)\n",
        "\n",
        "Gated Recurrent Units (GRUs) are a type of RNN that was introduced in 2014 by Kyunghyun Cho et al. GRUs are similar to LSTMs, but they have a much simpler structure and are computationally less expensive. GRUs have two gates - an update gate and a reset gate - that control the flow of information in the network. The update gate determines how much of the new information from the current input should be added to the hidden state, while the reset gate determines how much of the information from the previous hidden state should be forgotten.\n",
        "\n",
        "GRUs are computationally less expensive than LSTMs, which makes them more suitable for large-scale NLP tasks. GRUs are also simpler in structure compared to LSTMs, which makes them easier to understand and implement. However, LSTMs have a larger capacity to capture long-term dependencies in sequences compared to GRUs. Therefore, the choice between GRUs and LSTMs depends on the complexity of the NLP task and the computational resources available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_4 (Embedding)     (None, 120, 50)           400000    \n",
            "                                                                 \n",
            " gru (GRU)                   (None, 64)                22272     \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 64)                4160      \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 428,545\n",
            "Trainable params: 428,545\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "gru_model = tf.keras.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "        tf.keras.layers.GRU(64, activation=\"tanh\"),\n",
        "        tf.keras.layers.Dense(64, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(32, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "gru_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1407/1407 [==============================] - 48s 34ms/step - loss: 0.2218 - accuracy: 0.9108 - val_loss: 0.4170 - val_accuracy: 0.8240\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x2f7edcac0>"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# compile the model\n",
        "gru_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# train the model\n",
        "gru_model.fit(\n",
        "    x_train, y_train, epochs=10, batch_size=batch_size, validation_data=(x_test, y_test)\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bidirectional RNN Model <a name=\"bidirectional-rnn-model\"></a>\n",
        "[Back to Top](#toc)\n",
        "\n",
        "Bidirectional Recurrent Neural Networks (RNNs) are a variant of RNNs that have the ability to process inputs in both forward and backward directions, allowing the model to effectively capture context from both past and future words in a text sequence.\n",
        "\n",
        "The motivation behind Bidirectional RNNs is that, in natural language processing tasks, the meaning of a word can be influenced by both the preceding and succeeding words. For example, consider the word \"not\" in the sentence \"I do not like it.\" The word \"not\" influences the meaning of the word \"like,\" making it its opposite. A traditional RNN model would only have access to context from preceding words, but a Bidirectional RNN would have access to both the preceding and succeeding words.\n",
        "\n",
        "In Keras, a Bidirectional RNN can be implemented by wrapping an RNN layer in a Bidirectional layer. For example, if you have a simple LSTM layer defined as `LSTM(64)`, to make it bidirectional, you can wrap it like this: `Bidirectional(LSTM(64))`.\n",
        "\n",
        "Bidirectional RNNs are often used in NLP tasks such as sentiment analysis, named entity recognition, and machine translation, where capturing context from both past and future words is important. However, they are computationally more expensive compared to traditional RNNs and should be used judiciously based on the size and complexity of the task at hand."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_5 (Embedding)     (None, 120, 50)           400000    \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 128)              44544     \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 454,913\n",
            "Trainable params: 454,913\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "bi_gru_model = tf.keras.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64, activation=\"tanh\")),\n",
        "        tf.keras.layers.Dense(64, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(32, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "bi_gru_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-02-04 20:19:46.039900: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
            "2023-02-04 20:19:46.327065: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
            "2023-02-04 20:19:46.339504: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
            "2023-02-04 20:19:47.117449: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
            "2023-02-04 20:19:47.132845: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1407/1407 [==============================] - ETA: 0s - loss: 0.4316 - accuracy: 0.7916"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-02-04 20:21:17.107310: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
            "2023-02-04 20:21:17.208134: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
            "2023-02-04 20:21:17.215530: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1407/1407 [==============================] - 97s 67ms/step - loss: 0.4316 - accuracy: 0.7916 - val_loss: 0.4219 - val_accuracy: 0.8024\n",
            "Epoch 2/10\n",
            "1407/1407 [==============================] - 89s 63ms/step - loss: 0.3489 - accuracy: 0.8532 - val_loss: 0.3361 - val_accuracy: 0.8620\n",
            "Epoch 3/10\n",
            "1407/1407 [==============================] - 87s 62ms/step - loss: 0.2918 - accuracy: 0.8809 - val_loss: 0.3278 - val_accuracy: 0.8600\n",
            "Epoch 4/10\n",
            "1407/1407 [==============================] - 86s 61ms/step - loss: 0.2954 - accuracy: 0.8788 - val_loss: 0.3347 - val_accuracy: 0.8578\n",
            "Epoch 5/10\n",
            "1407/1407 [==============================] - 86s 61ms/step - loss: 0.2741 - accuracy: 0.8900 - val_loss: 0.3394 - val_accuracy: 0.8570\n",
            "Epoch 6/10\n",
            "1407/1407 [==============================] - 85s 61ms/step - loss: 0.2588 - accuracy: 0.8983 - val_loss: 0.3441 - val_accuracy: 0.8614\n",
            "Epoch 7/10\n",
            "1407/1407 [==============================] - 85s 61ms/step - loss: 0.2422 - accuracy: 0.9049 - val_loss: 0.3680 - val_accuracy: 0.8508\n",
            "Epoch 8/10\n",
            "1407/1407 [==============================] - 85s 61ms/step - loss: 0.2342 - accuracy: 0.9072 - val_loss: 0.3565 - val_accuracy: 0.8560\n",
            "Epoch 9/10\n",
            "1407/1407 [==============================] - 85s 60ms/step - loss: 0.2274 - accuracy: 0.9109 - val_loss: 0.3611 - val_accuracy: 0.8542\n",
            "Epoch 10/10\n",
            "1407/1407 [==============================] - 87s 62ms/step - loss: 0.2158 - accuracy: 0.9158 - val_loss: 0.3799 - val_accuracy: 0.8350\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x2ce62de80>"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# compile the model\n",
        "bi_gru_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# train the model\n",
        "bi_gru_model.fit(\n",
        "    x_train, y_train, epochs=10, batch_size=batch_size, validation_data=(x_test, y_test)\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ML-Training",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "effefe004be52d4cd3a12856ff0d4a1b800b83fc4bd48cce66e2ad043e78af0c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}