{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Data Pipelines\n",
    "\n",
    "{{ badge }}\n",
    "\n",
    "In this notebook, we'll cover the basics of building data pipelines for text data. This is an important step in processing text data efficiently and effectively for tasks such as sentiment analysis or machine translation.\n",
    "\n",
    "We'll start by reading text data from directories, preprocessing the data to clean and prepare it for modeling, and then building a data pipeline to efficiently process the data and feed it into a machine learning model.\n",
    "\n",
    "By the end of this notebook, you will have a good understanding of how to build and use text data pipelines in practice."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents <a name=\"toc\"></a>\n",
    "- [Text Dataset from Directory](#text-dataset-from-directory)\n",
    "- [Text Vectorization Layer](#text-vectorization-layer)\n",
    "- [Model Training](#model-training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "84125825/84125825 [==============================] - 35s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Download the dataset\n",
    "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "# we will be using the tf.keras.utils.get_file method to download the dataset and extract it automatically\n",
    "dataset = tf.keras.utils.get_file(\n",
    "    \"aclImdb\", url, untar=True, cache_dir=\".\", cache_subdir=\"\"\n",
    ")\n",
    "\n",
    "# remove extra class that we will not be using\n",
    "!rm -rf aclImdb/train/unsup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the data is structured as follows:\n",
    "\n",
    "```\n",
    "aclImdb\n",
    "├── test\n",
    "│   ├── neg\n",
    "|   |   ├── 0_2.txt\n",
    "|   |   ├── 10000_4.txt\n",
    "|   |   ├── ...\n",
    "│   └── pos\n",
    "|       ├── 0_10.txt\n",
    "|       ├── 10000_7.txt\n",
    "|       ├── ...\n",
    "└── train\n",
    "    ├── neg\n",
    "    |   ├── 0_3.txt\n",
    "    |   ├── 10000_4.txt\n",
    "    |   ├── ...\n",
    "    └── pos\n",
    "        ├── 0_8.txt\n",
    "        ├── 10000_7.txt\n",
    "        ├── ...\n",
    "```\n",
    "\n",
    "This is similar to cats vs dogs dataset structure we used in the previous week, but instead of images, we have text files where each file contains a review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we will set some parameters\n",
    "vocab_size = 8000  # number of words in the vocabulary, we will use the top 8000 most common words\n",
    "max_length = 120  # maximum length of a review, we will truncate reviews longer than 120 words and pad reviews shorter than 120 words\n",
    "embedding_dim = 50  # dimension of the embedding vector, we will use 50-dimensional embedding vectors\n",
    "batch_size = 32  # number of reviews in each batch\n",
    "seed = 42  # random seed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Dataset from Directory <a name=\"text-dataset-from-directory\"></a>\n",
    "[Back to top](#toc)\n",
    "\n",
    "The `tf.keras.utils.text_dataset_from_directory` is a function that provides the ability to read and process text data from a directory. It's ideal for working with large datasets of text data that are organized in a directory structure.\n",
    "\n",
    "This class has the following key parameters:\n",
    "\n",
    "- `directory`: The directory containing the text data. The files in this directory will be processed as text data.\n",
    "- `labels`: A list of labels, one for each text file. The labels should correspond to the text files in the directory. The default value is `infered` which will infer the labels from the directory structure.\n",
    "- `label_mode`: The type of label to return. The default value is `int` which will return an integer label for each text file. The other options are `binary` which will return a binary label for each text file, and `categorical` which will return a categorical label for each text file.\n",
    "\n",
    "For full documentation, see the [tf.keras.utils.text_dataset_from_directory doc](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text_dataset_from_directory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# read the train and test datasets\n",
    "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/train\", batch_size=32, seed=seed\n",
    ")\n",
    "\n",
    "raw_test_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/test\", batch_size=32, seed=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: tf.Tensor(b'\"Pandemonium\" is a horror movie spoof that comes off more stupid than funny. Believe me when I tell you, I love comedies. Especially comedy spoofs. \"Airplane\", \"The Naked Gun\" trilogy, \"Blazing Saddles\", \"High Anxiety\", and \"Spaceballs\" are some of my favorite comedies that spoof a particular genre. \"Pandemonium\" is not up there with those films. Most of the scenes in this movie had me sitting there in stunned silence because the movie wasn\\'t all that funny. There are a few laughs in the film, but when you watch a comedy, you expect to laugh a lot more than a few times and that\\'s all this film has going for it. Geez, \"Scream\" had more laughs than this film and that was more of a horror film. How bizarre is that?<br /><br />*1/2 (out of four)', shape=(), dtype=string)\n",
      "Label: tf.Tensor(0, shape=(), dtype=int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-05 02:43:40.935256: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "# preview samples from the training dataset\n",
    "for text_batch, label_batch in raw_train_ds.take(1):\n",
    "    print(\"Review:\", text_batch[0])\n",
    "    print(\"Label:\", label_batch[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Vectorization Layer <a name=\"text-vectorization-layer\"></a>\n",
    "[Back to top](#toc)\n",
    "\n",
    "Text Vectorization is a preprocessing step in NLP where we convert the raw text data into numerical representations or embeddings. This is a crucial step as most machine learning models work with numerical data, not with text.\n",
    "\n",
    "In TensorFlow and Keras, there are several layers that we can use for text vectorization. We have already used `tf.keras.preprocessing.text.Tokenizer` in the previous lesson which only tokenizes the text data. In this lesson, we will use `tf.keras.layers.experimental.preprocessing.TextVectorization` which is a more advanced layer which will take care of the whole preprocessing pipeline.\n",
    "\n",
    "The `TextVectorization` layer has the following key parameters:\n",
    "- `max_tokens`: The maximum number of words to keep, based on word frequency. Only the most common `max_tokens` words will be kept.\n",
    "- `output_mode`: The output mode of the layer. The default value is `int` which will return an integer representation of the words. The other options are `binary` which will return a binary representation of the words, and `count` which will return the count of each word.\n",
    "- `output_sequence_length`: The length of the output sequences. If the input sequence is shorter than this value, the output sequence will be padded. If the input sequence is longer than this value, the output sequence will be truncated.\n",
    "- `standardize`: The standardization to apply to the text. The default value is `lower_and_strip_punctuation` which will convert the text to lowercase and strip punctuation. This parameter can be set to a custom function to apply a custom standardization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a custom standardization function similar to the one we used in the previous notebook but using TensorFlow operations\n",
    "def custom_standardization(text):\n",
    "    # change all text to lowercase\n",
    "    text = tf.strings.lower(text)\n",
    "\n",
    "    # remove HTML tags\n",
    "    text = tf.strings.regex_replace(text, r\"<.*?>\", \"\")\n",
    "\n",
    "    # remove numbers\n",
    "    text = tf.strings.regex_replace(text, r\"\\d+\", \"\")\n",
    "\n",
    "    # remove words with numbers\n",
    "    text = tf.strings.regex_replace(text, r\"\\w*\\d\\w*\", \"\")\n",
    "\n",
    "    # remove URLs\n",
    "    text = tf.strings.regex_replace(text, r\"https?://\\S+\", \"\")\n",
    "\n",
    "    # remove emails\n",
    "    text = tf.strings.regex_replace(text, r\"\\S+@\\S+\", \"\")\n",
    "\n",
    "    # remove mentions (@username)\n",
    "    text = tf.strings.regex_replace(text, r\"@\\S+\", \"\")\n",
    "\n",
    "    # remove hashtags (#)\n",
    "    text = tf.strings.regex_replace(text, r\"#\", \"\")\n",
    "\n",
    "    # remove Punctuation\n",
    "    text = tf.strings.regex_replace(text, f\"[{re.escape(string.punctuation)}]\", \" \")\n",
    "\n",
    "    # remove extra spaces\n",
    "    text = tf.strings.regex_replace(text, r\"\\s+\", \" \")\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-05 02:43:45.637438: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "# create a TextVectorization layer with our custom standardization function and other parameters\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length,\n",
    ")\n",
    "\n",
    "# make a text-only dataset (without labels), then call adapt to build the vocabulary\n",
    "train_text = raw_train_ds.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pipeline mapping function to vectorize the text and label\n",
    "def vectorize_text(text, label):\n",
    "    # expand the dimensions of the text to make it into a batch\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    # apply the vectorization layer to the text\n",
    "    text = vectorize_layer(text)\n",
    "    return text, label\n",
    "\n",
    "\n",
    "# create a pipeline mapping function to vectorize the text and label\n",
    "def dataset_creator(dataset):\n",
    "    # create a dataset of text and labels\n",
    "    dataset = dataset.map(\n",
    "        vectorize_text, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "    )\n",
    "\n",
    "    # prefetch the dataset to improve latency\n",
    "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# create the training and test datasets\n",
    "train_ds = dataset_creator(raw_train_ds)\n",
    "test_ds = dataset_creator(raw_test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X batch shape: (32, 120) Y batch shape: (32,)\n",
      "X: tf.Tensor(\n",
      "[  84   18  256    2  223    1  566   32  232   11 2436    1   54   22\n",
      "   28  413  254   12  315  278    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0], shape=(120,), dtype=int64)\n",
      "Y: tf.Tensor(0, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# preview samples from the training dataset\n",
    "for x_batch, y_batch in train_ds.take(1):\n",
    "    print(\"X batch shape:\", x_batch.shape, \"Y batch shape:\", y_batch.shape)\n",
    "    print(\"X:\", x_batch[0])\n",
    "    print(\"Y:\", y_batch[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training <a name=\"model-training\"></a>\n",
    "[Back to top](#toc)\n",
    "\n",
    "In this section, we will build a model to classify the text data using the data pipeline we built in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 120, 50)           400000    \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 64)                22272     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 428,545\n",
      "Trainable params: 428,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gru_model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "        tf.keras.layers.GRU(64, activation=\"tanh\"),\n",
    "        tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "gru_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "gru_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# train the model\n",
    "gru_model.fit(train_ds, epochs=10, validation_data=test_ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-Training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "effefe004be52d4cd3a12856ff0d4a1b800b83fc4bd48cce66e2ad043e78af0c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
