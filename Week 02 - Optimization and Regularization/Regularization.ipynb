{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlLPYUfJGcSK"
      },
      "source": [
        "# Regularization in TensorFlow\n",
        "\n",
        "<!--<badge>--><a href=\"https://colab.research.google.com/github/TheAIDojo/AI_4_Climate_Bootcamp/blob/main/Week 02 - Optimization and Regularization/Regularization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a><!--</badge>-->\n",
        "\n",
        "Regularization is a technique used to prevent overfitting in deep learning models. Overfitting occurs when a model is trained too well on the training data and performs poorly on unseen data. Regularization helps to reduce the complexity of the model and make it more generalizable to new data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iv_1SilGRHu"
      },
      "source": [
        "## Table of Contents <a name=\"toc\"></a>\n",
        "* [Preparing the Example](#prep)\n",
        "* [Dropout](#dropout)\n",
        "* [Early Stopping](#early-stopping)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4NGHPVHGmIH"
      },
      "source": [
        "## Preparing the Example <a name=\"prep\"></a>\n",
        "[Back to Top](#toc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aEngIJPwGJRr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jW_W_xzxEHJo",
        "outputId": "5b54fa01-4a27-423f-8673-dd3f977c2436"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((60000, 28, 28), (60000,), (10000, 28, 28), (60000,))"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load and preprocess the data\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "x_train.shape, y_train.shape, x_test.shape, y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "Fv4ewQxzESI5",
        "outputId": "d1f4047d-049f-4470-e4b5-9ff6f719d6fa"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Pick a random image index\n",
        "index = np.random.randint(0, x_train.shape[0])\n",
        "\n",
        "# Plot the image\n",
        "plt.imshow(x_train[index], cmap=\"gray\")\n",
        "plt.title(\"Label: \" + str(y_train[index]))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0iSgwwxELmV",
        "outputId": "54d4ba04-875e-4ac5-c7b1-3ecae18e16f6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((60000, 784), (10000, 784))"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# reshape the data to be in the format (batch_size, input_dim) because Dense layers require one dimensional input\n",
        "x_train = x_train.reshape(-1, 28 * 28)\n",
        "x_test = x_test.reshape(-1, 28 * 28)\n",
        "\n",
        "x_train.shape, x_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "E8sS-G2aEmUr"
      },
      "outputs": [],
      "source": [
        "# Normalize the data\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kONnV_mDEw6x",
        "outputId": "ba00e7ad-0a1b-4708-ef3d-ef233a402df4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(TensorShape([60000, 10]), TensorShape([10000, 10]))"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# convert the labels from integers to categorical using one hot encoding\n",
        "depth = len(set(y_train))  # calculate the number of classes to use as depth\n",
        "y_train = tf.one_hot(y_train, depth=depth)\n",
        "y_test = tf.one_hot(y_test, depth=depth)\n",
        "\n",
        "y_train.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVWMcjkEIlqj"
      },
      "source": [
        "## Dropout <a name=\"dropout\"></a>\n",
        "[Back to Top](#toc)\n",
        "\n",
        "Dropout is a regularization technique that helps to prevent overfitting in deep learning models. It works by randomly \"dropping out\" or ignoring a subset of neurons during training. This is done by setting a probability, usually between 0 and 1, for each neuron to be dropped out.\n",
        "\n",
        "When dropout is applied, each neuron in the network has a probability of p of being \"dropped out\" during training. This means that for each training iteration, the neuron will not be used to make any computations or contribute to the final output. By dropping out neurons at random during training, the model is forced to learn to rely on multiple neurons rather than just a few specific ones. This makes the model more robust to noise in the data and less reliant on any one neuron.\n",
        "\n",
        "Dropout can be applied to any layer of a neural network, but it is most commonly applied to the fully connected layers. In TensorFlow, dropout can be easily applied to a layer using the `tf.keras.layers.Dropout` layer, where you can specify the dropout rate(probability) as a parameter.\n",
        "\n",
        "Here is an example of how to apply dropout to a dense layer in TensorFlow:\n",
        "\n",
        "```\n",
        "model = tf.keras.Sequential([\n",
        "  ...\n",
        "  tf.keras.layers.Dense(64, activation='relu'),\n",
        "  tf.keras.layers.Dropout(rate=0.2)\n",
        "  ...\n",
        "])\n",
        "```\n",
        "This will apply a dropout rate of 0.2 (20%) to the dense layer, meaning that 20% of the neurons in this layer will be dropped out during training. It's worth noting that the dropout is only used during training time, during the inference time all neurons are used.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNBIUnOhFCVs",
        "outputId": "d95f52d3-afcf-42c0-9cdd-2ac3f9997057"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 256)               200960    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 235,146\n",
            "Trainable params: 235,146\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# fetch the input shape, i.e. the number of features\n",
        "input_shape = x_train.shape[1]  # input_shape = 784\n",
        "\n",
        "model = tf.keras.Sequential(\n",
        "    [\n",
        "        # input layer\n",
        "        tf.keras.layers.Input(shape=(input_shape)),\n",
        "        # hidden layers\n",
        "        tf.keras.layers.Dense(256, activation=\"relu\"),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "        # output layer\n",
        "        tf.keras.layers.Dense(10, activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "MmAZIsPVFS12"
      },
      "outputs": [],
      "source": [
        "loss_fn = tf.losses.CategoricalCrossentropy()\n",
        "optimizer = tf.optimizers.Adam(learning_rate=0.001)\n",
        "metrics = [tf.keras.metrics.CategoricalAccuracy()]\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=optimizer, loss=loss_fn, metrics=metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qM1_OmruFZu2",
        "outputId": "5026099a-5668-4304-afd2-5682857e3503"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 19s 9ms/step - loss: 0.2759 - categorical_accuracy: 0.9169 - val_loss: 0.1205 - val_categorical_accuracy: 0.9630\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 16s 9ms/step - loss: 0.1293 - categorical_accuracy: 0.9611 - val_loss: 0.0920 - val_categorical_accuracy: 0.9720\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0977 - categorical_accuracy: 0.9705 - val_loss: 0.0913 - val_categorical_accuracy: 0.9728\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 10s 6ms/step - loss: 0.0832 - categorical_accuracy: 0.9743 - val_loss: 0.0771 - val_categorical_accuracy: 0.9766\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0707 - categorical_accuracy: 0.9780 - val_loss: 0.0686 - val_categorical_accuracy: 0.9800\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0647 - categorical_accuracy: 0.9796 - val_loss: 0.0728 - val_categorical_accuracy: 0.9800\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0555 - categorical_accuracy: 0.9818 - val_loss: 0.0718 - val_categorical_accuracy: 0.9804\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0526 - categorical_accuracy: 0.9836 - val_loss: 0.0789 - val_categorical_accuracy: 0.9801\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0492 - categorical_accuracy: 0.9840 - val_loss: 0.0685 - val_categorical_accuracy: 0.9827\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0448 - categorical_accuracy: 0.9859 - val_loss: 0.0686 - val_categorical_accuracy: 0.9823\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1ec9527e20>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(x_train, y_train, batch_size=32, epochs=10, validation_data=(x_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7znRvZ2WJ1fj"
      },
      "source": [
        "## Early Stopping <a name=\"early-stopping\"></a>\n",
        "[Back to Top](#toc)\n",
        "\n",
        "Early stopping is a regularization technique used to prevent overfitting in deep learning models. The idea behind early stopping is to monitor the performance of the model on a validation set during training, and stop the training process when the performance starts to degrade.\n",
        "\n",
        "The validation set is a set of data that is used to evaluate the model's performance during training. The performance is typically measured by a metric such as accuracy or loss. The validation set is used to ensure that the model is generalizing well to new data, and not just memorizing the training data.\n",
        "\n",
        "During training, the performance of the model on the validation set is monitored at regular intervals (for example, after every epoch). If the performance of the model on the validation set stops improving for a certain number of consecutive iterations (for example, after several epochs), the training process is stopped. This helps to prevent the model from overfitting by stopping the training process before the model reaches its maximum number of training iterations.\n",
        "\n",
        "In TensorFlow, early stopping can be implemented by setting a `tf.keras.callbacks.EarlyStopping` callback during the training process, which monitors the performance of the model on a validation set and stops the training process when the performance stops improving. You can specify the metric to monitor, the number of iterations to wait before stopping the training, and whether to restore the weights of the best performing model.\n",
        "\n",
        "Here is an example of how to use early stopping in TensorFlow:\n",
        "\n",
        "```\n",
        "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "model.fit(x_train, y_train, batch_size=32, epochs=100, validation_data=(x_test, y_test), callbacks=[es])\n",
        "```\n",
        "\n",
        "In this example, we used a `tf.keras.callbacks.EarlyStopping` callback to monitor the validation loss, we set the patience to 5, that means the training will stop if the validation loss stop improving after 5 epochs.\n",
        "\n",
        "Early stopping is a simple yet effective regularization technique that can be used to prevent overfitting in deep learning models. It helps to ensure that the model is generalizing well to new data and not just memorizing the training data, by stopping the training process when the performance on the validation set starts to degrade.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igUjgjEE6RhO",
        "outputId": "fe8b4758-31b3-45d5-c448-49421a7a10dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_3 (Dense)             (None, 256)               200960    \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 235,146\n",
            "Trainable params: 235,146\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# fetch the input shape, i.e. the number of features\n",
        "input_shape = x_train.shape[1]  # input_shape = 784\n",
        "\n",
        "model = tf.keras.Sequential(\n",
        "    [\n",
        "        # input layer\n",
        "        tf.keras.layers.Input(shape=(input_shape)),\n",
        "        # hidden layers\n",
        "        tf.keras.layers.Dense(256, activation=\"relu\"),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "        # output layer\n",
        "        tf.keras.layers.Dense(10, activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "rf1duGcn6RhO"
      },
      "outputs": [],
      "source": [
        "loss_fn = tf.losses.CategoricalCrossentropy()\n",
        "optimizer = tf.optimizers.Adam(learning_rate=0.001)\n",
        "metrics = [tf.keras.metrics.CategoricalAccuracy()]\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=optimizer, loss=loss_fn, metrics=metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ncnolz876RhO",
        "outputId": "32edf916-c99b-403c-e44a-02baed809a7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "1875/1875 [==============================] - 9s 4ms/step - loss: 0.2709 - categorical_accuracy: 0.9184 - val_loss: 0.1101 - val_categorical_accuracy: 0.9660\n",
            "Epoch 2/20\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1252 - categorical_accuracy: 0.9618 - val_loss: 0.0966 - val_categorical_accuracy: 0.9698\n",
            "Epoch 3/20\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0989 - categorical_accuracy: 0.9697 - val_loss: 0.0728 - val_categorical_accuracy: 0.9785\n",
            "Epoch 4/20\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0801 - categorical_accuracy: 0.9748 - val_loss: 0.0776 - val_categorical_accuracy: 0.9776\n",
            "Epoch 5/20\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0698 - categorical_accuracy: 0.9782 - val_loss: 0.0710 - val_categorical_accuracy: 0.9777\n",
            "Epoch 6/20\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0613 - categorical_accuracy: 0.9808 - val_loss: 0.0743 - val_categorical_accuracy: 0.9795\n",
            "Epoch 7/20\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0566 - categorical_accuracy: 0.9826 - val_loss: 0.0713 - val_categorical_accuracy: 0.9787\n",
            "Epoch 8/20\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0541 - categorical_accuracy: 0.9827 - val_loss: 0.0857 - val_categorical_accuracy: 0.9781\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1ec948fc10>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_loss\", patience=3, mode=\"min\"\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    batch_size=32,\n",
        "    epochs=20,\n",
        "    validation_data=(x_test, y_test),\n",
        "    callbacks=[early_stopping],\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}